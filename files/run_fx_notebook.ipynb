{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe9fa69",
   "metadata": {},
   "source": [
    "# General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760bb038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import os\n",
    "import yaml\n",
    "import datetime\n",
    "from fx19 import inputs\n",
    "from fx19.run_ops import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff3f15c",
   "metadata": {},
   "source": [
    "# Initialize FANTASTX\n",
    "\n",
    "1. Read in the YAML file\n",
    "2. Create all FANTASTX objects\n",
    "3. Assign all FANTASTX objects to local variables which can be passed to the worker nodes\n",
    "4. Create a calculations folder where all model evaluations will take place\n",
    "5. Initialize the data file (helpfully called \"data_file\") which will store all model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = os.getcwd()\n",
    "# read input file and make input dictionary\n",
    "with open('input.yaml') as ifile:\n",
    "    i_dict = yaml.load(ifile, Loader=yaml.FullLoader)\n",
    "    i_dict['main_path'] = main_path\n",
    "\n",
    "# make objects\n",
    "all_objects = inputs.make_objects(i_dict)\n",
    "\n",
    "# Assign objects from all_objects to local variables\n",
    "reg_id = all_objects['reg_id']\n",
    "input_model_obj = all_objects['input_model_obj']\n",
    "gb_ops_obj = None\n",
    "if 'gb_ops_obj' in all_objects:\n",
    "    gb_ops_obj = all_objects['gb_ops_obj']\n",
    "# kwargs for full_eval() function\n",
    "if gb_ops_obj is not None:\n",
    "    random_model_obj = gb_ops_obj\n",
    "    evolve = gb_ops_obj\n",
    "else:\n",
    "    random_model_obj = all_objects['random_model_obj']\n",
    "    evolve = all_objects['evolve']\n",
    "\n",
    "energy_code = all_objects['energy_code']\n",
    "sim_ids = None\n",
    "if 'Xsim_1' in all_objects.keys():\n",
    "    Xsim_1 = all_objects['Xsim_1']\n",
    "    sim_ids = [1]\n",
    "else:\n",
    "    Xsim_1 = None\n",
    "    \n",
    "pool = all_objects['pool']\n",
    "select = all_objects['select']\n",
    "\n",
    "# Create a folder 'Calcs' where all calculations take place\n",
    "if 'calcs' in os.listdir(main_path):\n",
    "    now = datetime.datetime.now()\n",
    "    new_name = 'old_{}_{}_{}_{}_{}_{}'.format(now.year, now.month, now.day,\n",
    "                                              now.hour, now.minute, now.second)\n",
    "    os.rename('calcs', new_name)\n",
    "calcs = i_dict['main_path'] + '/calcs'\n",
    "os.mkdir(calcs)\n",
    "\n",
    "# Create data file where objective function values and inheritance\n",
    "# information will be written.\n",
    "data_file = main_path + '/data_file'\n",
    "with open(data_file, 'w') as f:\n",
    "    first_line = 'Label   Inheritance     Total Energy    Obj_0' + \\\n",
    "        '           Obj_1           Operator\\n\\n'\n",
    "    if not Xsim_1:\n",
    "        first_line = 'Label   Inheritance     Total Energy    Obj_0' + \\\n",
    "            '           Operator\\n\\n'\n",
    "    f.write(first_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88224147",
   "metadata": {},
   "source": [
    "# Initialize DASK and start the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster, PBSCluster\n",
    "from dask.distributed import Client\n",
    "import dask\n",
    "import dask.distributed\n",
    "dask.config.set({'distributed.comm.timeouts.tcp': '3h'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274c42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up everything for calculations\n",
    "models_evald = 0\n",
    "evald_futures, simd_futures = [], []\n",
    "pool_status_update = 10\n",
    "workers = i_dict['workers']\n",
    "max_workers = workers['max_workers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6bdd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Dask client\n",
    "if workers['cluster'] == 'SLURM':\n",
    "    job_script = '/home/dunruh/fantastx_vasp_xanes/sample_job_script.txt'\n",
    "    jobfile = open(job_script, \"w+\")\n",
    "    cluster_job = SLURMCluster(cores=workers['num_cores'],\n",
    "                               memory=workers['total_mem'],\n",
    "                               processes=workers['processes'],\n",
    "                               project=workers['project_name'],\n",
    "                               queue=workers['submit_queue'],\n",
    "                               interface=workers['node_type'],\n",
    "                               walltime=workers['walltime'],\n",
    "                               job_extra=workers['job_extra'],\n",
    "                               env_extra=workers['env_extra'],\n",
    "                               header_skip=workers['header_skip'])\n",
    "    print(\"Job script for dask-worker: \\n\", cluster_job.job_script())\n",
    "    client = Client(cluster_job)\n",
    "    # jobfile.write(cluster_job.job_script())\n",
    "    # jobfile.close()\n",
    "elif workers['cluster'] == 'PBS':\n",
    "    job_script = '/home/dunruh/sample_job_script.txt'\n",
    "    jobfile = open(job_script, \"w+\")\n",
    "    cluster_job = PBSCluster(cores=workers['num_cores'],\n",
    "                             memory=workers['total_mem'],\n",
    "                             project=workers['project_name'],\n",
    "                             interface=workers['node_type'],\n",
    "                             walltime=workers['walltime'],\n",
    "                             job_extra=workers['job_extra'],\n",
    "                             header_skip=workers['header_skip'])\n",
    "    print(\"Job script for dask-worker: \\n\", cluster_job.job_script())\n",
    "    client = Client(cluster_job)\n",
    "elif workers['cluster'] == 'local':\n",
    "    client = Client('tcp://127.0.0.1:8786')\n",
    "else:\n",
    "    print('FANTASTX currently supports SLURM, PBS and local. Provided '\n",
    "          'scheduler type not identified.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d1c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if workers['cluster'] == 'SLURM' or workers['cluster'] == 'PBS':\n",
    "    cluster_job.adapt(minimum=max_workers, maximum=max_workers)\n",
    "    cluster_job.adapt(minimum_jobs=max_workers, maximum_jobs=max_workers)\n",
    "    client = Client(cluster_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d7078",
   "metadata": {},
   "source": [
    "#### Retrieve the client dashboard in order to track and visualize the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb17d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for workers to start on cluster\n",
    "client.wait_for_workers(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13342f18",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Define the eval function which will create a new candidate model and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090c5959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_eval(model_obj, evolve, select, pool, reg_id, model_type, model):\n",
    "    \"\"\"\n",
    "    A wrapper function around energy_eval and Xsim_eval.\n",
    "    Both these are done one after the other as one job by worker\n",
    "\n",
    "    Args:\n",
    "    model - (obj) Newly created model object which shall be evaluated\n",
    "\n",
    "    Note:\n",
    "    Uses reg_id, Xsim_1, energy_code objects which were stored as global\n",
    "    parameters in all workers and master\n",
    "    \"\"\"\n",
    "    new_model = make_model(random_model_obj, evolve, select, pool,\n",
    "                                    reg_id, model_type, model)\n",
    "    \n",
    "    relaxed_model = relax(new_model, reg_id, energy_code)\n",
    "\n",
    "    if relaxed_model is None:\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Model converged: {model.converged}\")\n",
    "\n",
    "    # separate gb_iface for the energy evaluated futures\n",
    "    separate_gb(energy_code, gb_ops_obj, model)\n",
    "    \n",
    "    exp_eval_model = do_Xsim(relaxed_model, Xsim_1)\n",
    "        \n",
    "    return exp_eval_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112902b0",
   "metadata": {},
   "source": [
    "# Run FANTASTX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533da872",
   "metadata": {},
   "source": [
    "## Make models directly from input POSCARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new model from all input files provided, then random, then evolve\n",
    "input_models = []\n",
    "if input_model_obj is not None:\n",
    "    for i in range(len(input_model_obj.all_files)):\n",
    "        new_model = input_model_obj.read_structure(reg_id)\n",
    "        if new_model is not None:\n",
    "            # read_structure() returns 0 when all files are done\n",
    "            if not isinstance(new_model, int):\n",
    "                input_models.append(new_model)\n",
    "\n",
    "    # evaluate the input models\n",
    "    for input_model in input_models:\n",
    "#         new_model, select = make_model(random_model_obj, evolve, select, pool,\n",
    "#                                        reg_id, model_type='inputs',\n",
    "#                                        model=input_model)\n",
    "        # relax the model in dask-workers\n",
    "        out = client.submit(create_and_eval, random_model_obj, evolve, select, pool,\n",
    "                            reg_id, model_type = 'inputs', model = input_model)\n",
    "        evald_futures.append(out)\n",
    "        print(\n",
    "            f\"Successfully submitted input model {new_model.label}\")\n",
    "        \n",
    "num_initial_pop = i_dict['population_limits']['initial_population']\n",
    "total_models_needed = i_dict['population_limits']['total_population']\n",
    "evald_futures, models_evald, pool, select = update_pool(evald_futures,\n",
    "                                                        models_evald,\n",
    "                                                        pool, select,\n",
    "                                                        data_file,\n",
    "                                                        sim_ids)\n",
    "working_jobs = get_working_jobs(evald_futures)\n",
    "\n",
    "print('Input models are finished. Current working jobs: {working_jobs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab4b7dc",
   "metadata": {},
   "source": [
    "## Made random and evolved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a5255",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "while models_evald < total_models_needed:\n",
    "    # to lower cpu usage, wait 10 seconds before updating\n",
    "    # number of working jobs each loop\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Grab number of workers which are currently busy\n",
    "    working_jobs = get_working_jobs(evald_futures)\n",
    "    \n",
    "    # Add new jobs if any workers are idle and more models are still needed\n",
    "    while working_jobs < len(client.scheduler_info()['workers']) and models_evald < total_models_needed:\n",
    "        # New models will either be random models or evolved models\n",
    "        if models_evald < num_initial_pop:\n",
    "            model_mech = 'random'\n",
    "            print(\"Submitting random job\")\n",
    "        else:\n",
    "            model_mech = 'evolved'\n",
    "            print(\"Submitting evolved job\")\n",
    "            \n",
    "        new_model = make_model(random_model_obj, evolve, select,\n",
    "                                           pool, reg_id, model_type=model_mech)\n",
    "\n",
    "        # relax the model in dask-workers\n",
    "        out = client.submit(create_and_eval, new_model)\n",
    "        evald_futures.append(out)\n",
    "        evald_futures, models_evald, pool, select = update_pool(evald_futures,\n",
    "                                                                models_evald,\n",
    "                                                                pool, select,\n",
    "                                                                data_file,\n",
    "                                                                sim_ids)\n",
    "        working_jobs = get_working_jobs(evald_futures)\n",
    "\n",
    "        if models_evald % pool_status_update == 0 and\\\n",
    "                models_evald >= i_dict['population_limits']['pool']:\n",
    "            # print statements which output visualization information\n",
    "            if \"selection_algorithm\" in i_dict[\"select_params\"]:\n",
    "                if i_dict[\"select_params\"][\"selection_algorithm\"] ==\\\n",
    "                        \"distance_from_pareto\":\n",
    "                    good_pool = pool.good_pool\n",
    "                    good_pool_labels = [model.label for model in good_pool]\n",
    "                    print(\n",
    "                        \"Current good_pool population models: \"\n",
    "                        f\"{good_pool_labels}.\")\n",
    "                elif i_dict[\"select_params\"][\"selection_algorithm\"] ==\\\n",
    "                        \"epsilon_moea\":\n",
    "                    pop_labels = [\n",
    "                        model.label for model in pool.population.models]\n",
    "                    archive_labels = [\n",
    "                        model.label for model in pool.archive.models]\n",
    "                    print(\"Current pool population models: \"\n",
    "                          f\"{pop_labels}\")\n",
    "                    print(\"Current pool archive models:\"\n",
    "                          f\"{archive_labels}\")\n",
    "                elif i_dict[\"select_params\"][\"selection_algorithm\"] ==\\\n",
    "                        \"clustered_selection\":\n",
    "                    nd_pop_labels = [\n",
    "                        model.label for\n",
    "                        model in pool.population.non_dominated_models]\n",
    "                    print(\n",
    "                        \"Current pool population non-dominated models: \"\n",
    "                        f\"{nd_pop_labels}\")\n",
    "            else:\n",
    "                good_pool = pool.good_pool\n",
    "                good_pool_labels = [model.label for model in good_pool]\n",
    "                print(\n",
    "                    \"Current good_pool population models: \"\n",
    "                    f\"{good_pool_labels}.\")\n",
    "                \n",
    "# process extra calculations running in last batch\n",
    "while len(evald_futures) > 0:\n",
    "    evald_futures, models_evald, pool, select = update_pool(evald_futures,\n",
    "                                                            models_evald,\n",
    "                                                            pool, select,\n",
    "                                                            data_file, sim_ids)\n",
    "    \n",
    "    \n",
    "# print statements which output visualization information\n",
    "if \"selection_algorithm\" in i_dict[\"select_params\"]:\n",
    "    if i_dict[\"select_params\"][\"selection_algorithm\"] ==\\\n",
    "            \"distance_from_pareto\":\n",
    "        good_pool = pool.good_pool\n",
    "        good_pool_labels = [model.label for model in good_pool]\n",
    "        print(f\"Current good_pool population models: {good_pool_labels}.\")\n",
    "    elif i_dict[\"select_params\"][\"selection_algorithm\"] == \"epsilon_moea\":\n",
    "        pop_labels = [model.label for model in pool.population.models]\n",
    "        archive_labels = [model.label for model in pool.archive.models]\n",
    "        print(f\"Current pool population models: {pop_labels}\")\n",
    "        print(f\"Current pool archive models: {archive_labels}\")\n",
    "    elif i_dict[\"select_params\"][\"selection_algorithm\"] ==\\\n",
    "            \"clustered_selection\":\n",
    "        nd_pop_labels = [\n",
    "            model.label for model in pool.population.non_dominated_models]\n",
    "        print(f\"Current pool population non-dominated models: {nd_pop_labels}\")\n",
    "else:\n",
    "    good_pool = pool.good_pool\n",
    "    good_pool_labels = [model.label for model in good_pool]\n",
    "    print(f\"Current good_pool population models: {good_pool_labels}.\")\n",
    "    \n",
    "print(f\"Current operator probabilities: {select.operator_frequencies}\")\n",
    "print('Done!')\n",
    "print('Total time: ', time.time() - start_time)\n",
    "\n",
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "2cc43449510ba6361eb1a0b0b12a5303c3c6c8dfa7f0cc85cd09440e8110c08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
